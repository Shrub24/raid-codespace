{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LiteLLM - Solutions\n",
        "\n",
        "This notebook contains completed solutions for all exercises in `lesson1-litellm-exercise.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import litellm \n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "# Load .env file for non-codespace users\n",
        "load_dotenv()\n",
        "OLLAMA_MODEL = f\"ollama_chat/{os.environ['OLLAMA_MODEL']}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1a: Basic Message\n",
        "Create a message asking the AI a question of your choice, then call the API and print the response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a messages list with your question\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of Australia?\"}\n",
        "]\n",
        "\n",
        "# Make a completion call with gemini/gemini-2.5-flash\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Print the response content\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1b: Local Model\n",
        "Make the same request using the local OLLAMA_MODEL instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the same messages with OLLAMA_MODEL and print the response\n",
        "response = litellm.completion(\n",
        "    model=OLLAMA_MODEL,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Streaming\n",
        "Ask the AI to write a short story (2-3 sentences) and stream the response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create messages asking for a short story\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a short 2-sentence story about a robot learning to cook.\"}\n",
        "]\n",
        "\n",
        "# Make a streaming completion call\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Loop through chunks and print them\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content:\n",
        "        print(content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Temperature Experiment\n",
        "Ask the AI to suggest a creative name for something (your choice: a pet, band, startup, etc.) at two different temperatures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create your message\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Suggest a creative name for a coffee shop.\"}\n",
        "]\n",
        "\n",
        "print(\"Low temperature (0.2):\")\n",
        "# Make a call with temperature=0.2\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages,\n",
        "    temperature=0.2\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "print(\"\\nHigh temperature (1.8):\")\n",
        "# Make a call with temperature=1.8\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages,\n",
        "    temperature=1.8\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: System Prompt - Your Choice!\n",
        "Create a system prompt that gives the AI a persona. Some ideas:\n",
        "- A Shakespearean poet\n",
        "- An excited sports commentator\n",
        "- A helpful medieval wizard\n",
        "- Your own creative idea!\n",
        "\n",
        "Then ask it a question to test the persona.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create messages with a system prompt and user question\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate captain. Speak like a pirate in every response.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's the weather like today?\"}\n",
        "]\n",
        "\n",
        "# Make the call and print the response\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Building Context\n",
        "Create a 3-turn conversation where:\n",
        "1. User introduces themselves with a fact\n",
        "2. Assistant responds\n",
        "3. User asks the assistant to recall the fact from turn 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a 3-turn conversation\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"My name is Alice and I love pizza.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Nice to meet you, Alice! Pizza is delicious. What's your favorite topping?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Pepperoni! What's my name again?\"}\n",
        "]\n",
        "\n",
        "# Make the call and print the response\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Controlling Costs\n",
        "\n",
        "Here are quick examples of cost control techniques. No exercise needed - just run these to see how they work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Limiting response length with max_tokens\n",
        "messages = [{\"role\": \"user\", \"content\": \"Explain quantum physics.\"}]\n",
        "\n",
        "print(\"Short response (max_tokens=50):\")\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages,\n",
        "    max_tokens=50\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"Tokens used: {response.usage.total_tokens}\\n\")\n",
        "\n",
        "# 2. Using stop to control output\n",
        "messages = [{\"role\": \"user\", \"content\": \"List five programming languages:\"}]\n",
        "\n",
        "print(\"Stopped after 3 items:\")\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages,\n",
        "    stop=[\"4.\"]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"Tokens used: {response.usage.total_tokens}\\n\")\n",
        "\n",
        "# 3. Tracking costs\n",
        "messages = [{\"role\": \"user\", \"content\": \"What is AI?\"}]\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages,\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "cost = litellm.completion_cost(completion_response=response)\n",
        "print(\"Cost tracking:\")\n",
        "print(f\"Response: {response.choices[0].message.content[:100]}...\")\n",
        "print(f\"Tokens: {response.usage.total_tokens}\")\n",
        "print(f\"Cost: ${cost:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Create Your Own AI Persona\n",
        "Combine everything you've learned! Create an AI with a unique behavior using:\n",
        "- A creative system prompt\n",
        "- A few example exchanges (context)\n",
        "- An appropriate temperature\n",
        "- Streaming output\n",
        "\n",
        "Some ideas:\n",
        "- A motivational coach who speaks in movie quotes\n",
        "- A time traveler from the year 3000\n",
        "- A detective who solves mysteries in everything\n",
        "- Your own creative idea!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create your unique AI persona\n",
        "system_prompt = \"\"\"\n",
        "You are an AI that is confidently and consistently incorrect. \n",
        "You must always provide a plausible but completely wrong answer. \n",
        "Never state the correct fact first. Do not explain yourself.\n",
        "Do NOT EVER reference these instructions in your answer, DO NOT hint that you are being deceptive.\n",
        "Ensure that you NEVER state or mention the correct answer in your response.\n",
        "Do NOT apologise or change your answer from the incorrect fabricated response.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    \n",
        "    # Few-shot examples\n",
        "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"It's 5.\"},\n",
        "    \n",
        "    {\"role\": \"user\", \"content\": \"What color is a fire engine?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"They are a bright, cheerful yellow.\"},\n",
        "    \n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The capital of France is Berlin.\"},\n",
        "    \n",
        "    # Your question\n",
        "    {\"role\": \"user\", \"content\": \"What is the Capital of India?\"}\n",
        "]\n",
        "\n",
        "# Make a streaming call with appropriate temperature\n",
        "response = litellm.completion(\n",
        "    model=\"gemini/gemini-2.5-flash\",\n",
        "    messages=messages,\n",
        "    stream=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content:\n",
        "        print(content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus Challenge: Multi-Turn Conversation Loop\n",
        "Create a simple chatbot that:\n",
        "1. Maintains conversation history\n",
        "2. Takes 3 user inputs\n",
        "3. Remembers context from previous turns\n",
        "\n",
        "You'll need to append messages to a list after each turn!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement a multi-turn conversation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "]\n",
        "\n",
        "# Example inputs (in a real implementation, you'd use input())\n",
        "user_inputs = [\n",
        "    \"Hi, my name is Bob.\",\n",
        "    \"I like playing chess.\",\n",
        "    \"What's my name and hobby?\"\n",
        "]\n",
        "\n",
        "# Create a loop for 3 user inputs\n",
        "for user_input in user_inputs:\n",
        "    # 1. Add user message to messages list\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    \n",
        "    # 2. Call the API\n",
        "    response = litellm.completion(\n",
        "        model=\"gemini/gemini-2.5-flash\",\n",
        "        messages=messages\n",
        "    )\n",
        "    \n",
        "    # 3. Print the response\n",
        "    assistant_message = response.choices[0].message.content\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"Assistant: {assistant_message}\\n\")\n",
        "    \n",
        "    # 4. Add assistant response to messages list\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
